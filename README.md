# Price Scraper Dashboard

Sistema de web scraping para rastrear precios de productos en Amazon y Mercado Libre con visualizaciÃ³n de tendencias histÃ³ricas.

## CaracterÃ­sticas

- ğŸ” Scraping de precios de Amazon y Mercado Libre
- ğŸ“Š Dashboard web con grÃ¡ficos de tendencia de precios
- ğŸ’¾ Almacenamiento histÃ³rico en base de datos SQLite
- â° EjecuciÃ³n automÃ¡tica diaria programada
- ğŸš€ API REST con FastAPI
- ğŸ“ˆ VisualizaciÃ³n con Chart.js

## TecnologÃ­as Utilizadas

**Backend:**
- Python 3.10+
- FastAPI - API REST
- SQLAlchemy - ORM
- BeautifulSoup4 - Web scraping
- APScheduler - Tareas programadas
- SQLite - Base de datos

**Frontend:**
- HTML/CSS/JavaScript
- Chart.js - GrÃ¡ficos

## InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone <repository-url>
cd web-scrapping
```

### 2. Crear entorno virtual

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar variables de entorno

Copia el archivo `.env.example` y renÃ³mbralo a `.env`, luego ajusta los valores segÃºn tus necesidades.

## Uso

### Iniciar el servidor

```bash
python -m uvicorn backend.api.main:app --reload
```

El servidor estarÃ¡ disponible en `http://localhost:8000`

### Acceder al dashboard

Abre tu navegador y visita:
- Dashboard: `http://localhost:8000/`
- API Docs (Swagger): `http://localhost:8000/docs`

### AÃ±adir productos para rastrear

1. Abre el dashboard
2. Usa el formulario "AÃ±adir Producto"
3. Ingresa la URL del producto (Amazon o Mercado Libre)
4. El sistema detectarÃ¡ automÃ¡ticamente la plataforma y comenzarÃ¡ a rastrear

### Scraping manual

Puedes ejecutar el scraping manualmente desde:
- Dashboard: BotÃ³n "Ejecutar Scraping Ahora"
- API: `POST http://localhost:8000/scrape/run`

### Scraping automÃ¡tico

El sistema estÃ¡ configurado para ejecutar scraping automÃ¡ticamente todos los dÃ­as a las 8:00 AM (configurable en `.env`).

## Estructura del Proyecto

```
web-scrapping/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ base.py            # Clase base para scrapers
â”‚   â”‚   â”œâ”€â”€ mercadolibre.py    # Scraper de Mercado Libre
â”‚   â”‚   â””â”€â”€ amazon.py          # Scraper de Amazon
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ models.py          # Modelos SQLAlchemy
â”‚   â”‚   â””â”€â”€ db.py              # ConexiÃ³n y operaciones
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py            # FastAPI endpoints
â”‚   â”œâ”€â”€ scheduler/
â”‚   â”‚   â””â”€â”€ jobs.py            # Tareas programadas
â”‚   â””â”€â”€ config.py              # ConfiguraciÃ³n
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html             # Dashboard principal
â”‚   â”œâ”€â”€ styles.css             # Estilos
â”‚   â””â”€â”€ app.js                 # LÃ³gica del frontend
â”œâ”€â”€ data/
â”‚   â””â”€â”€ prices.db              # Base de datos SQLite
â””â”€â”€ requirements.txt           # Dependencias Python
```

## API Endpoints

- `GET /` - Dashboard web
- `GET /products` - Listar productos rastreados
- `POST /products` - AÃ±adir nuevo producto
- `DELETE /products/{id}` - Eliminar producto
- `GET /products/{id}/history` - HistÃ³rico de precios
- `POST /scrape/run` - Ejecutar scraping manual
- `GET /stats` - EstadÃ­sticas generales

## Consideraciones Legales

- **Mercado Libre**: Este proyecto usa la API oficial pÃºblica de Mercado Libre
- **Amazon**: El scraping de Amazon debe usarse solo para fines educativos y personales
- **Rate Limiting**: El sistema respeta delays entre requests para no sobrecargar los servidores
- **Robots.txt**: Se respetan las directivas de cada sitio

## Mejoras Futuras

- [ ] Sistema de alertas por email/Telegram
- [ ] Comparador de precios entre plataformas
- [ ] Exportar datos a CSV/Excel
- [ ] AutenticaciÃ³n de usuarios
- [ ] Soporte para mÃ¡s plataformas (eBay, AliExpress)

## Contribuciones

Las contribuciones son bienvenidas. Por favor, abre un issue primero para discutir los cambios que te gustarÃ­a hacer.

## Licencia

Este proyecto es solo para fines educativos y uso personal.

